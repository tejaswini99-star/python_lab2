# -*- coding: utf-8 -*-
"""Lab2_que1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KobyCyT7HZkqXxPhQCD_aCXK-9_-m5xy
"""

# Regression Example With Boston Dataset: Standardized and Wider
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense,Activation
# from keras.wrappers.scikit_learn import KerasRegressor
# from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import keras
import keras.backend as kb
import tensorflow as tf
from keras.utils import np_utils
from keras.callbacks import TensorBoard
import matplotlib.pyplot as plt

housing_data = pd.read_csv("housing.csv")
housing_data

features = housing_data[['LSTAT','RM','PTRATIO']]
label = housing_data['MEDV'].values
label = label.reshape(-1,1)

print(features.shape)
print(label.shape)

label = housing_data['MEDV']
features = housing_data.drop(['MEDV'],axis=1)
X, X_test, y, y_test = train_test_split(features,label,test_size=0.2,train_size=0.8,random_state=5)
X_train, X_validation, y_train, y_validation = train_test_split(X,y,test_size = 0.25,train_size=0.75,random_state=5)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_validation_scaled = scaler.fit_transform(X_validation)
X_test_scaled  = scaler.fit_transform(X_test)

from keras.activations import relu, elu, selu, sigmoid, exponential, tanh

# Hyper Parameters
optimizer  = tf.keras.optimizers.RMSprop(0.0099)
activation = 'relu'
epochs = 500
batch_size = 50

model = Sequential()
model.add(Dense(32, input_shape=(3,))) # 3 is number of features
model.add(Activation(activation))
model.add(Dense(32, input_shape=(3,)))
model.add(Activation(activation))
model.add(Dense(32, input_shape=(3,)))
model.add(Activation(activation))
model.add(Dense(1))
model.compile(loss='mean_squared_error', metrics=['accuracy'], optimizer=optimizer)

tensorboard = TensorBoard(log_dir='1', histogram_freq=0, write_graph=True, write_images=False)
history=model.fit(X_train_scaled,y_train,epochs=epochs, batch_size=batch_size, shuffle=True,validation_data=(X_validation,y_validation),  callbacks=[tensorboard])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fig1 = plt.figure()
plt.plot(history.history['loss'], 'r', linewidth=3.0)
plt.plot(history.history['val_loss'], 'b', linewidth=3.0)
plt.legend(['Training loss', 'Validation Loss'], fontsize=18)
plt.xlabel('Epochs ', fontsize=16)
plt.ylabel('Loss', fontsize=16)
plt.title('Loss Curves : ', fontsize=16)
fig1.savefig('loss_lstm.png')

score, accuracy = model.evaluate(X_test_scaled, y_test, batch_size=batch_size, verbose=0)
print("Test fraction correct (NN-Score) = {:.2f}".format(score))
print("Test fraction correct (NN-Accuracy) = {:.2f}".format(accuracy))

print("[INFO] predicting house prices...")
preds = model.predict(X_test_scaled)
diff = preds.flatten() - y_test
percentDiff = (diff / y_test) * 100
absPercentDiff = np.abs(percentDiff)
mean = np.mean(absPercentDiff)
std = np.std(absPercentDiff)
print("[INFO] avg. MEDV: {}, std MEDV: {}".format(
	(y_test.mean()),
	(y_test.std())))
print("[INFO] avg. predicted MEDV: {}, std predicted MEDV: {}".format(
	(preds.mean()),
	(preds.std())))
print("[INFO] mean: {:.2f}%, std: {:.2f}%".format(mean, std))